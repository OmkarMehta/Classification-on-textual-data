{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining: Classification analysis on textual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our brains can easily perceive the meaning of the word in the context it is used. How will a machine learning algorithm do the same? This idea is quite interesting, ain't it? Suppose it's a library and we ask librarian to sort out 10,000 books by its genre. It would take a couple of days to do so. What if we have 10,000 digital books and we ask our algorithm to do the sorting? Is it possible? Yes, it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and what it represents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've used 20 newsgroups dataset for classification analysis of textual data. You can download the dataset from [here.](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has been coded using Python 3.7.1 under environment Anaconda Jupyter Notebook.\n",
    "Please install the following packages before running the code.\n",
    "#### 1.  nltk v3.2.2\n",
    "#### 2. numpy v1.11.3\n",
    "#### 3. matplotlib v2.0.0\n",
    "#### 4. sklearn v0.18.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Model Text Data and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm can't read textual data. So, textual data needs to be encoded as integers or floating point values. \n",
    "Let's go through step-by-step. \n",
    "\n",
    "Step 1: Tokenization: Parsing text to remove words. \n",
    "\n",
    "Step 2: Feature extraction (or vectorization): Encoding words as integers to be fed as input to algorithm.\n",
    "\n",
    "Read more about how to do this [here](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "\n",
    "Read about 'Bag of words' and 'TFIDF' method. This basically helps us converting textual data into a vectorised array of floating values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import dataset from scikit library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are total of 20 classes. You can find the [list of classes here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) \n",
    "\n",
    "For the sake of understanding, we will consider only the following four classes: \n",
    "\n",
    "    'comp.graphics',\n",
    "\n",
    "    'comp.os.ms-windows.misc',\n",
    " \n",
    "    'comp.sys.ibm.pc.hardware',\n",
    " \n",
    "    'comp.sys.mac.hardware'.\n",
    " \n",
    "**Since this is about computer technology, we will henceforth consider only one class: 'Computer Technology'. The abovementioned will be four subclasses.**\n",
    "\n",
    "Let's then make a list of these subclasses as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_technology_subclasses=['comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computer_technology_subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "We need to make train and test data for the above class. This is quite easy to make.\n",
    "If you look at docstring of fetch_20newsgroups under, you can set subset to either 'train'/'test'/'all'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_20newsgroups in module sklearn.datasets.twenty_newsgroups:\n",
      "\n",
      "fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n",
      "    Load the filenames and data from the 20 newsgroups dataset (classification).\n",
      "    \n",
      "    Download it if necessary.\n",
      "    \n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "    \n",
      "    Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data_home : optional, default: None\n",
      "        Specify a download and cache folder for the datasets. If None,\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    subset : 'train' or 'test', 'all', optional\n",
      "        Select the dataset to load: 'train' for the training set, 'test'\n",
      "        for the test set, 'all' for both, with shuffled ordering.\n",
      "    \n",
      "    categories : None or collection of string or unicode\n",
      "        If None (default), load all the categories.\n",
      "        If not None, list of category names to load (other categories\n",
      "        ignored).\n",
      "    \n",
      "    shuffle : bool, optional\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    random_state : int, RandomState instance or None (default)\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    remove : tuple\n",
      "        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "        these are kinds of text that will be detected and removed from the\n",
      "        newsgroup posts, preventing classifiers from overfitting on\n",
      "        metadata.\n",
      "    \n",
      "        'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "        ends of posts that look like signatures, and 'quotes' removes lines\n",
      "        that appear to be quoting another post.\n",
      "    \n",
      "        'headers' follows an exact standard; the other filters are not always\n",
      "        correct.\n",
      "    \n",
      "    download_if_missing : optional, True by default\n",
      "        If False, raise an IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    bunch : Bunch object\n",
      "        bunch.data: list, length [n_samples]\n",
      "        bunch.target: array, shape [n_samples]\n",
      "        bunch.filenames: list, length [n_classes]\n",
      "        bunch.DESCR: a description of the dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also need to remove headers, footers, quotes, punctuations, stop words(eg., the, and, or). Stemming is used to remove suffixes of similar words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming train and test data\n",
    "computer_train=fetch_20newsgroups(subset='train',categories=computer_technology_subclasses,shuffle=True,random_state=42,\n",
    "                                  remove=('headers','footers','quotes'))\n",
    "computer_test=fetch_20newsgroups(subset='test',categories=computer_technology_subclasses,shuffle=True,random_state=42,\n",
    "                                 remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x23d9d63a550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the stemmer to be used in preprocessing the data\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[! \" # $ % \\\\& \\' \\\\( \\\\) \\\\ * + , \\\\- \\\\. \\\\/ : ; <=> ? @ \\\\[ \\\\ \\\\] ^ _ ` { \\\\| } ~]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the list of punctutations to be trimmed off the data in the preprocessing stage\n",
    "punctuations='[! \\\" # $ % \\& \\' \\( \\) \\ * + , \\- \\. \\/ : ; <=> ? @ \\[ \\\\ \\] ^ _ ` { \\| } ~]'\n",
    "punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "#You can find more information by using Shift+Tab on re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for stemming, and removing punctuations\n",
    "def preprocess(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i]=\" \".join([stemmer.stem(data) for data in re.split(punctuations,data[i])])\n",
    "        data[i]=data[i].replace('\\n','').replace('\\t','').replace('\\r','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the two datasets\n",
    "preprocess(computer_train.data)\n",
    "preprocess(computer_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(computer_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CountVectorizer** provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
    "\n",
    "The **TfidfVectorizer** will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned **CountVectorizer**, you can use it with a **TfidfTransformer** to just calculate the inverse document frequencies and start encoding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our tfidf matrix. \n",
    "\n",
    "CountVectorizer will create the vocabulary of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the instance of CountVectorizer class and removing stop_words.\n",
    "#min_df = 2 means that if any word occurs rarely or has frequency of occurence lower than 2, it will be considered irrelevant\n",
    "# and discarded.\n",
    "vectorizer=CountVectorizer(min_df=2,stop_words ='english')\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3903x17594 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 190613 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize and build vocab\n",
    "vectorizer_counts=vectorizer.fit_transform(computer_train.data+computer_test.data)\n",
    "vectorizer_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse matrix means that the matrix will have a lot of zeros in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lc',\n",
       " 'iii',\n",
       " 'pop',\n",
       " 'reveal',\n",
       " 'socket',\n",
       " 'addit',\n",
       " 'vram',\n",
       " '72',\n",
       " 'pin',\n",
       " 'ram',\n",
       " 'flat',\n",
       " 'pack',\n",
       " 'processor',\n",
       " 'direct',\n",
       " 'slot',\n",
       " 'pds',\n",
       " 'ident',\n",
       " 'ii',\n",
       " 'withan',\n",
       " 'set',\n",
       " '32',\n",
       " 'path',\n",
       " 'guess',\n",
       " 'board',\n",
       " 'powerpc',\n",
       " 'chip',\n",
       " 'onli',\n",
       " 'place',\n",
       " 'hi',\n",
       " 'believ',\n",
       " 'undocu',\n",
       " 'featur',\n",
       " 'window',\n",
       " 'directori',\n",
       " 'microsoft',\n",
       " 'diagnost',\n",
       " 'ver',\n",
       " '00',\n",
       " 'specif',\n",
       " 'depth',\n",
       " 'explan',\n",
       " 'legend',\n",
       " 'memori',\n",
       " 'map',\n",
       " 'report',\n",
       " 'thank',\n",
       " 'dh',\n",
       " 'doe',\n",
       " 'anyon',\n",
       " 'mountain',\n",
       " 'tape',\n",
       " 'backup',\n",
       " 'note',\n",
       " 'jumper',\n",
       " 'softwar',\n",
       " 'know',\n",
       " 'contact',\n",
       " 'maker',\n",
       " 'drive',\n",
       " 'network',\n",
       " 'solut',\n",
       " '800',\n",
       " '458',\n",
       " '0300',\n",
       " 'general',\n",
       " 'number',\n",
       " '408',\n",
       " '438',\n",
       " 'tech',\n",
       " 'support',\n",
       " 'bbs',\n",
       " 'veri',\n",
       " 'info',\n",
       " 'david',\n",
       " 'especi',\n",
       " 'line',\n",
       " 'bye',\n",
       " 'slmr',\n",
       " '1a',\n",
       " 'hobbi',\n",
       " 'good',\n",
       " 'model',\n",
       " 'packag',\n",
       " 'irit',\n",
       " 'look',\n",
       " 'tar',\n",
       " 'howev',\n",
       " 'convert',\n",
       " 'format',\n",
       " 'pov',\n",
       " 'request',\n",
       " 'group',\n",
       " 'got',\n",
       " 'respons',\n",
       " 'consid',\n",
       " 'write',\n",
       " 'program',\n",
       " 'perhap',\n",
       " 'tri',\n",
       " 'nic',\n",
       " 'funet',\n",
       " 'fi',\n",
       " 'instead',\n",
       " 'biggest',\n",
       " 'ftp',\n",
       " 'site',\n",
       " 'europ',\n",
       " 'butth',\n",
       " 'stuff',\n",
       " 'avail',\n",
       " 'big',\n",
       " 'pool',\n",
       " 'run',\n",
       " 'ms',\n",
       " 'dos',\n",
       " 'localtalk',\n",
       " 'hook',\n",
       " 'daystar',\n",
       " 'digit',\n",
       " 'mc',\n",
       " 'ps',\n",
       " '70',\n",
       " 'use',\n",
       " 'appleshar',\n",
       " 'file',\n",
       " 'server',\n",
       " 'access',\n",
       " 'work',\n",
       " 'fine',\n",
       " 'thewindow',\n",
       " 'os',\n",
       " 'box',\n",
       " 'load',\n",
       " 'befor',\n",
       " 'thateveri',\n",
       " 'applic',\n",
       " 'machin',\n",
       " 'crash',\n",
       " 'hard',\n",
       " 'start',\n",
       " 'cours',\n",
       " 'thing',\n",
       " 'pretti',\n",
       " 'experi',\n",
       " 'bizarr',\n",
       " 'obsolet',\n",
       " 'setup',\n",
       " 'telix',\n",
       " 'kermit',\n",
       " 'win',\n",
       " 'exit',\n",
       " 'windowsto',\n",
       " 'serial',\n",
       " 'port',\n",
       " 'idea',\n",
       " 'howto',\n",
       " 'solv',\n",
       " 'problem',\n",
       " 'inform',\n",
       " 'send',\n",
       " 'emailor',\n",
       " 'news',\n",
       " 'dale',\n",
       " 'erickson',\n",
       " 'derick',\n",
       " 'plain',\n",
       " 'nodak',\n",
       " 'edu',\n",
       " 'regard',\n",
       " 'post',\n",
       " 'mac',\n",
       " 'portabl',\n",
       " 'powerbook',\n",
       " '100',\n",
       " 'coupl',\n",
       " 'answer',\n",
       " 'appletalk',\n",
       " 'yes',\n",
       " 'cpu',\n",
       " 'low',\n",
       " 'power',\n",
       " 'version',\n",
       " '68000',\n",
       " '15',\n",
       " '87',\n",
       " 'mhz',\n",
       " 'lot',\n",
       " 'sever',\n",
       " 'vendor',\n",
       " 'make',\n",
       " 'option',\n",
       " 'sell',\n",
       " 'plug',\n",
       " 'insid',\n",
       " 'way',\n",
       " 'address',\n",
       " 'megabyt',\n",
       " 'depend',\n",
       " 'backlit',\n",
       " 'king',\n",
       " 'peripheri',\n",
       " 'irvin',\n",
       " 'california',\n",
       " 'cheapest',\n",
       " '2mb',\n",
       " '140',\n",
       " 'mb',\n",
       " '250',\n",
       " '415',\n",
       " 'ad',\n",
       " 'macus',\n",
       " 'modul',\n",
       " '450',\n",
       " 'wide',\n",
       " 'varieti',\n",
       " 'price',\n",
       " 'intern',\n",
       " 'hd',\n",
       " 'conner',\n",
       " 'cp',\n",
       " '40',\n",
       " 'averag',\n",
       " 'time',\n",
       " '25',\n",
       " 'screen',\n",
       " 'better',\n",
       " 'pb',\n",
       " 'passiv',\n",
       " 'matrix',\n",
       " 'mean',\n",
       " 'fade',\n",
       " 'head',\n",
       " 'degre',\n",
       " 'left',\n",
       " 'right',\n",
       " 'non',\n",
       " 'activ',\n",
       " 'think',\n",
       " 'joy',\n",
       " 'read',\n",
       " 'pay',\n",
       " 'supplier',\n",
       " 'mass',\n",
       " '2400',\n",
       " 'bps',\n",
       " 'modem',\n",
       " '650',\n",
       " '900',\n",
       " 'll',\n",
       " 'fax',\n",
       " 'anybodi',\n",
       " 'want',\n",
       " 'luck',\n",
       " 'gene',\n",
       " 'wright',\n",
       " 've',\n",
       " 'attempt',\n",
       " 'adob',\n",
       " 'type',\n",
       " 'font',\n",
       " 'triangul',\n",
       " 'bevel',\n",
       " 'extrud',\n",
       " 'result',\n",
       " 'object',\n",
       " 'current',\n",
       " 'stuck',\n",
       " 'algorithm',\n",
       " 'ani',\n",
       " 'arbitrari',\n",
       " 'polygon',\n",
       " 'shape',\n",
       " 'delaunay',\n",
       " 'limitedto',\n",
       " 'convex',\n",
       " 'hull',\n",
       " 'constrain',\n",
       " 'okay',\n",
       " 'anoth',\n",
       " 'variat',\n",
       " 'creat',\n",
       " 'pictur',\n",
       " 'text',\n",
       " 'need',\n",
       " 'postscript',\n",
       " 'preview',\n",
       " 'white',\n",
       " 'black',\n",
       " 'exact',\n",
       " 'imag',\n",
       " 'open',\n",
       " 'close',\n",
       " 'mirror',\n",
       " 'compound',\n",
       " 'origin',\n",
       " 'exampl',\n",
       " 'union',\n",
       " 'height',\n",
       " 'field',\n",
       " 'gif',\n",
       " 'scale',\n",
       " 'textur',\n",
       " 'glass',\n",
       " 'translat',\n",
       " 'center',\n",
       " 'rotat',\n",
       " '90',\n",
       " '10',\n",
       " 'bigger',\n",
       " 'final',\n",
       " 'placement',\n",
       " 'size',\n",
       " 'contain',\n",
       " 'intim',\n",
       " 'roman',\n",
       " '256',\n",
       " 'point',\n",
       " 'just',\n",
       " 'becaus',\n",
       " '68070',\n",
       " 'upto',\n",
       " 'doesn',\n",
       " 'cd',\n",
       " 'speed',\n",
       " 'said',\n",
       " 'understand',\n",
       " 'runningat',\n",
       " 'someth',\n",
       " 'like',\n",
       " 'sure',\n",
       " 'longtim',\n",
       " 'ago',\n",
       " 'sprite',\n",
       " 'trick',\n",
       " 'cool',\n",
       " 'awesom',\n",
       " 'game',\n",
       " 'psygnosi',\n",
       " 'follow',\n",
       " 'everyth',\n",
       " 'advertis',\n",
       " 'intel',\n",
       " '400',\n",
       " '14',\n",
       " '4k',\n",
       " 'pc',\n",
       " 'unix',\n",
       " 'end',\n",
       " 'limit',\n",
       " '19',\n",
       " 'com',\n",
       " 'link',\n",
       " 'quantit',\n",
       " 'comparison',\n",
       " 'did',\n",
       " 'cat',\n",
       " '20',\n",
       " 'kbyte',\n",
       " 'uncompress',\n",
       " 'ascii',\n",
       " 'took',\n",
       " '75',\n",
       " 'second',\n",
       " 'scroll',\n",
       " '270',\n",
       " 'char',\n",
       " 'sec',\n",
       " 'hardwar',\n",
       " 'procomm',\n",
       " 'fw',\n",
       " '11',\n",
       " 'btw',\n",
       " 'ncd',\n",
       " 'xview',\n",
       " 'hp',\n",
       " 'everi',\n",
       " 'day',\n",
       " 'lan',\n",
       " 'connect',\n",
       " 'byte',\n",
       " '471',\n",
       " 'various',\n",
       " 'scenario',\n",
       " '1024x768x16',\n",
       " '18',\n",
       " '107',\n",
       " 'standard',\n",
       " 'vga',\n",
       " '1024x768x256',\n",
       " '30',\n",
       " 'seamless',\n",
       " 'gave',\n",
       " 'min',\n",
       " 'uw',\n",
       " 'faster',\n",
       " 'pg',\n",
       " 'littl',\n",
       " 'hit',\n",
       " 'space',\n",
       " 'bar',\n",
       " 'ati',\n",
       " 'wonder',\n",
       " 'xl',\n",
       " 'video',\n",
       " 'card',\n",
       " 'desktop',\n",
       " 'pathet',\n",
       " 'page',\n",
       " 'redraw',\n",
       " 'prettygood',\n",
       " 'normal',\n",
       " 'small',\n",
       " 'differ',\n",
       " 'receiv',\n",
       " 'light',\n",
       " 'solid',\n",
       " '9600',\n",
       " 'baud',\n",
       " 'draw',\n",
       " 'repost',\n",
       " 'summar',\n",
       " 'comment',\n",
       " 'long',\n",
       " '16mb',\n",
       " '386sx',\n",
       " 'math',\n",
       " 'coprocessor',\n",
       " 'anda',\n",
       " '120mb',\n",
       " '20mb',\n",
       " 'free',\n",
       " 'compress',\n",
       " 'enhanc',\n",
       " 'mode',\n",
       " '5mb',\n",
       " '1mb',\n",
       " 'smart',\n",
       " 'swap',\n",
       " 'perman',\n",
       " 'temporari',\n",
       " 'month',\n",
       " 'mathcad',\n",
       " 'concern',\n",
       " 'requir',\n",
       " 'mathsoft',\n",
       " 'claim',\n",
       " 'user',\n",
       " 'insist',\n",
       " 'far',\n",
       " 'descript',\n",
       " 'net',\n",
       " 'mail',\n",
       " 'appear',\n",
       " 'llnl',\n",
       " 'gov',\n",
       " 'accord',\n",
       " 'extens',\n",
       " 'physic',\n",
       " 'resourc',\n",
       " 'say',\n",
       " 'actual',\n",
       " 'win32',\n",
       " 'figur',\n",
       " '10mb',\n",
       " 'indic',\n",
       " 'minimum',\n",
       " 'bert',\n",
       " 'certain',\n",
       " 'culprit',\n",
       " '486',\n",
       " '66dx2',\n",
       " 'main',\n",
       " 'ramdisk',\n",
       " 'bit',\n",
       " 'session',\n",
       " 'startup',\n",
       " 'process',\n",
       " 'swapfil',\n",
       " 'instal',\n",
       " 'subsystem',\n",
       " 'march',\n",
       " 'beta',\n",
       " 'nt',\n",
       " 'sdk',\n",
       " 'demand',\n",
       " 'presenc',\n",
       " 'date',\n",
       " 'came',\n",
       " 'remain',\n",
       " 'fix',\n",
       " 'locat',\n",
       " 'disk',\n",
       " 'ece',\n",
       " 'cmu',\n",
       " 'brian',\n",
       " 'anderson',\n",
       " 'upgrad',\n",
       " 'windows',\n",
       " 'dure',\n",
       " 'told',\n",
       " 'student',\n",
       " 'tc',\n",
       " 'umn',\n",
       " 'steven',\n",
       " 'case',\n",
       " 'librari',\n",
       " 'probably',\n",
       " 'heard',\n",
       " 'provides',\n",
       " 'function',\n",
       " 'thread',\n",
       " 'multitask',\n",
       " 'chem',\n",
       " 'thoma',\n",
       " '16',\n",
       " 'inde',\n",
       " 'otherwis',\n",
       " 'fsu',\n",
       " 'greg',\n",
       " 'absolut',\n",
       " '4mb',\n",
       " '12mb',\n",
       " '8mb',\n",
       " 'error',\n",
       " 'major',\n",
       " 'hog',\n",
       " 'symbol',\n",
       " 'real',\n",
       " 'virtual',\n",
       " 'shown',\n",
       " 'possibl',\n",
       " 'choos',\n",
       " 'slower',\n",
       " 'appar',\n",
       " 'ship',\n",
       " 'uxa',\n",
       " 'ecn',\n",
       " 'bgu',\n",
       " 'meg',\n",
       " 'whi',\n",
       " 'configur',\n",
       " '10meg',\n",
       " 'permit',\n",
       " 'bootup',\n",
       " 'don',\n",
       " 'altern',\n",
       " 'provid',\n",
       " 'rememb',\n",
       " 'discuss',\n",
       " 'issu',\n",
       " 'thought',\n",
       " 'involv',\n",
       " 'reason',\n",
       " 'oneself',\n",
       " 'wild',\n",
       " 'digex',\n",
       " 'presum',\n",
       " 'exist',\n",
       " 'wouldn',\n",
       " 'happi',\n",
       " 'bought',\n",
       " 'bernoulli',\n",
       " 'year',\n",
       " 'compar',\n",
       " 'diffrent',\n",
       " 'storag',\n",
       " 'optic',\n",
       " 'slow',\n",
       " '13',\n",
       " 'sinc',\n",
       " 'onlin',\n",
       " 'quicktim',\n",
       " 'ran',\n",
       " 'fast',\n",
       " 'imho',\n",
       " 'best',\n",
       " 'buy',\n",
       " 'cartridg',\n",
       " 'quick',\n",
       " 'mani',\n",
       " 'explain',\n",
       " 'copi',\n",
       " 'protect',\n",
       " 'thati',\n",
       " 'unabl',\n",
       " 'handl',\n",
       " 'high',\n",
       " 'densiti',\n",
       " 'old',\n",
       " 'shit',\n",
       " 'surpris',\n",
       " 'hear',\n",
       " 'huh',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'question',\n",
       " 'troubl',\n",
       " 'wordperfect',\n",
       " 'select',\n",
       " 'chang',\n",
       " 'disappear',\n",
       " 'onc',\n",
       " 'correct',\n",
       " 'newsgroup',\n",
       " 'prefer',\n",
       " 'els',\n",
       " 'wait',\n",
       " 'nake',\n",
       " 'gun',\n",
       " 'pi',\n",
       " 'hello',\n",
       " 'fellow',\n",
       " 'netter',\n",
       " 'ask',\n",
       " 'collect',\n",
       " 'wisdom',\n",
       " 'help',\n",
       " 'decid',\n",
       " 'printer',\n",
       " 'purchas',\n",
       " 'canon',\n",
       " 'bj200',\n",
       " 'bubblejet',\n",
       " 'vs',\n",
       " 'deskjet',\n",
       " '500',\n",
       " 'trust',\n",
       " 'salesperson',\n",
       " 'benefit',\n",
       " 'daili',\n",
       " 'great',\n",
       " 'appreci',\n",
       " 'render',\n",
       " '360dpi',\n",
       " 'hewlett',\n",
       " 'packard',\n",
       " '300',\n",
       " 'dpi',\n",
       " 'notic',\n",
       " 'print',\n",
       " 'qualiti',\n",
       " 'particular',\n",
       " 'graphic',\n",
       " 'larg',\n",
       " 'document',\n",
       " 'person',\n",
       " 'driver',\n",
       " 'cost',\n",
       " 'toner',\n",
       " 'basic',\n",
       " 'yourperson',\n",
       " 'desir',\n",
       " 'bad',\n",
       " 'advanc',\n",
       " 'input',\n",
       " 'accept',\n",
       " 'encourag',\n",
       " 'bandwidth',\n",
       " 'sincer',\n",
       " 'robert',\n",
       " 'kayman',\n",
       " 'cs',\n",
       " 'stanford',\n",
       " 'cpa',\n",
       " 'relat',\n",
       " '1304',\n",
       " 'centri',\n",
       " 'multipl',\n",
       " 'resolut',\n",
       " 'curious',\n",
       " 'vl',\n",
       " 'bus',\n",
       " 'base',\n",
       " 'et4000',\n",
       " 'w32',\n",
       " 'come',\n",
       " 'morethan',\n",
       " 'perform',\n",
       " 'quantum',\n",
       " 'prodriv',\n",
       " '80at',\n",
       " 'ide',\n",
       " 'harddisk',\n",
       " 'wouldlik',\n",
       " 'fdisk',\n",
       " 'mess',\n",
       " 'enter',\n",
       " 'mbr',\n",
       " 'suggest',\n",
       " '38',\n",
       " '21mb',\n",
       " 'notth',\n",
       " 'nomin',\n",
       " '80mb',\n",
       " '47',\n",
       " 'cylind',\n",
       " 'someon',\n",
       " 'togeth',\n",
       " 'pleas',\n",
       " 'repli',\n",
       " 'email',\n",
       " 'gerthd',\n",
       " 'mvs',\n",
       " 'sas',\n",
       " 'comthank',\n",
       " 'thomas',\n",
       " 'anim',\n",
       " 'pro',\n",
       " 'quit',\n",
       " 'import',\n",
       " 'hope',\n",
       " 'author',\n",
       " 'daniel',\n",
       " 'background',\n",
       " 'pattern',\n",
       " 'edit',\n",
       " 'control',\n",
       " 'ini',\n",
       " 'add',\n",
       " 'section',\n",
       " 'brick',\n",
       " '148',\n",
       " '43',\n",
       " '86',\n",
       " '172',\n",
       " '89',\n",
       " '182',\n",
       " '99',\n",
       " '85',\n",
       " 'diagon',\n",
       " '54',\n",
       " '156',\n",
       " '73',\n",
       " '170',\n",
       " 'color',\n",
       " 'dark',\n",
       " 'grey',\n",
       " 'past',\n",
       " 'winter',\n",
       " 'spend',\n",
       " 'ridicul',\n",
       " 'comput',\n",
       " 'eye',\n",
       " 'shell',\n",
       " 'money',\n",
       " '17',\n",
       " 'monitor',\n",
       " 'hz',\n",
       " 'grate',\n",
       " 'usinga',\n",
       " 'smaller',\n",
       " 'strain',\n",
       " 'kind',\n",
       " 'appl',\n",
       " 'duo',\n",
       " 'enabl',\n",
       " 'somekind',\n",
       " 'sleep',\n",
       " 'investig',\n",
       " 'public',\n",
       " 'domain',\n",
       " 'assist',\n",
       " 'autocad',\n",
       " 'dxf',\n",
       " 'expans',\n",
       " 'takeadvantag',\n",
       " 'lciii',\n",
       " 'data',\n",
       " '25mhz',\n",
       " 'clock',\n",
       " 'ifthey',\n",
       " 'signific',\n",
       " 'lcii',\n",
       " 'agre',\n",
       " 'fair',\n",
       " 'complain',\n",
       " 'sgi',\n",
       " 'newer',\n",
       " 'architectur',\n",
       " 'older',\n",
       " 'dec',\n",
       " 'statement',\n",
       " 'mere',\n",
       " 'computercompani',\n",
       " 'constant',\n",
       " 'improv',\n",
       " 'product',\n",
       " 'marketposit',\n",
       " 'share',\n",
       " 'eventu',\n",
       " 'theyhav',\n",
       " 'replac',\n",
       " 'thesystem',\n",
       " 'incomput',\n",
       " 'lifetim',\n",
       " 'felt',\n",
       " 'histori',\n",
       " 'industri',\n",
       " 'opinion',\n",
       " 'mip',\n",
       " 'decstat',\n",
       " 'abandon',\n",
       " 'alpha',\n",
       " 'r5',\n",
       " '3mhz',\n",
       " 'transfer',\n",
       " 'cycl',\n",
       " 'realli',\n",
       " 'toperform',\n",
       " 'reduc',\n",
       " 'rate',\n",
       " 'common',\n",
       " 'isa',\n",
       " 'ibeliev',\n",
       " 'state',\n",
       " 'spec',\n",
       " 'obvious',\n",
       " 'quot',\n",
       " 'gripe',\n",
       " 'tiff',\n",
       " 'complic',\n",
       " 'infinit',\n",
       " 'easier',\n",
       " 'pd',\n",
       " 'minut',\n",
       " 'app',\n",
       " 'martin',\n",
       " 'archi',\n",
       " 'filenam',\n",
       " 'greet',\n",
       " 'faq',\n",
       " 'juli',\n",
       " '1992',\n",
       " 'articl',\n",
       " 'bitmap',\n",
       " 'manipul',\n",
       " 'perspect',\n",
       " 'rec',\n",
       " 'programm',\n",
       " 'includ',\n",
       " 'sourc',\n",
       " 'code',\n",
       " 'turbo',\n",
       " 'pascal',\n",
       " 'assembl',\n",
       " 'languag',\n",
       " 'archiv',\n",
       " 'abov',\n",
       " 'let',\n",
       " 'shadow',\n",
       " 'mask',\n",
       " 'face',\n",
       " 'persist',\n",
       " 'vision',\n",
       " 'raytrac',\n",
       " 'vp',\n",
       " 'wuarchiv',\n",
       " 'freewar',\n",
       " 'v32',\n",
       " 'v32bis',\n",
       " 'v42',\n",
       " 'v42bis',\n",
       " 'talk',\n",
       " 'itat',\n",
       " 'clear',\n",
       " 'complet',\n",
       " 'seper',\n",
       " 'hous',\n",
       " 'imposs',\n",
       " 'thismuch',\n",
       " 'pain',\n",
       " 'turn',\n",
       " 'hour',\n",
       " 'movi',\n",
       " 'toset',\n",
       " 'comm',\n",
       " 'stupid',\n",
       " 'garbag',\n",
       " '16550afn',\n",
       " 'uart',\n",
       " 'snot',\n",
       " '400bps',\n",
       " 'instruct',\n",
       " 'respond',\n",
       " 'ok',\n",
       " 'command',\n",
       " 'cando',\n",
       " 'precis',\n",
       " 'manual',\n",
       " 'unclear',\n",
       " 'didn',\n",
       " 'initi',\n",
       " '42bis',\n",
       " '42',\n",
       " 'mnp5',\n",
       " 'test',\n",
       " 'zip',\n",
       " 'string',\n",
       " 'formi',\n",
       " 'charg',\n",
       " 'offic',\n",
       " 'ihav',\n",
       " 'winword',\n",
       " 'excel',\n",
       " 'updat',\n",
       " 'singl',\n",
       " 'necessari',\n",
       " 'anyth',\n",
       " 'awar',\n",
       " 'deal',\n",
       " 'sorri',\n",
       " 'skeptic',\n",
       " '39',\n",
       " 'sound',\n",
       " 'suspici',\n",
       " 'mayb',\n",
       " 'university',\n",
       " 'dealer',\n",
       " 'shouldn',\n",
       " 'level',\n",
       " '1st',\n",
       " 'alreadi',\n",
       " 'automat',\n",
       " 'assign',\n",
       " 'nd',\n",
       " '2nd',\n",
       " 'partit',\n",
       " 'probabl',\n",
       " 'assum',\n",
       " 'armi',\n",
       " 'effici',\n",
       " 'r3000',\n",
       " 'integ',\n",
       " 'calcul',\n",
       " 'compact',\n",
       " 'codec',\n",
       " 'easi',\n",
       " 'task',\n",
       " '48',\n",
       " 'frame',\n",
       " '320x240',\n",
       " '5000',\n",
       " '200',\n",
       " 'burkhard',\n",
       " 'neideck',\n",
       " 'lutz',\n",
       " 'yep',\n",
       " 'alchemi',\n",
       " 'dac',\n",
       " 'rememberread',\n",
       " 'display',\n",
       " 'noth',\n",
       " 'flag',\n",
       " 'mad',\n",
       " 'camp',\n",
       " 'clarkson',\n",
       " 'univers',\n",
       " 'ford',\n",
       " 'asciicharact',\n",
       " 'platform',\n",
       " 'independ',\n",
       " 'workshift',\n",
       " 'karl',\n",
       " 'latest',\n",
       " 'later',\n",
       " 'cica',\n",
       " 'wrksft16',\n",
       " 'v1',\n",
       " 'tester',\n",
       " 'gain',\n",
       " 'workspac',\n",
       " 'technician',\n",
       " 'theplac',\n",
       " 'recent',\n",
       " 'guy',\n",
       " 'simm',\n",
       " 'custom',\n",
       " 'themselv',\n",
       " 'babi',\n",
       " 'zap',\n",
       " 'easili',\n",
       " 'static',\n",
       " 'electr',\n",
       " 'hsk',\n",
       " 'compani',\n",
       " 'keyboard',\n",
       " 'bio',\n",
       " 'andaft',\n",
       " 'went',\n",
       " 'seri',\n",
       " 'defect',\n",
       " 'michael',\n",
       " 'vesa',\n",
       " 'local',\n",
       " 'ncr',\n",
       " 'diamond',\n",
       " 'stealth',\n",
       " '24',\n",
       " 'vlb',\n",
       " 'cardfor',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "#Getting feature names\n",
    "vectorizer.vocabulary_.keys()\n",
    "vocab = list()\n",
    "for i in vectorizer.vocabulary_.keys():\n",
    "    vocab.append(i)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a dictionary of words and indices. Each word is a key and a number is assigned to each key as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lc</th>\n",
       "      <th>iii</th>\n",
       "      <th>pop</th>\n",
       "      <th>reveal</th>\n",
       "      <th>socket</th>\n",
       "      <th>addit</th>\n",
       "      <th>vram</th>\n",
       "      <th>72</th>\n",
       "      <th>pin</th>\n",
       "      <th>ram</th>\n",
       "      <th>...</th>\n",
       "      <th>sympathi</th>\n",
       "      <th>samea</th>\n",
       "      <th>genlock</th>\n",
       "      <th>infam</th>\n",
       "      <th>auxiliari</th>\n",
       "      <th>64mb</th>\n",
       "      <th>432</th>\n",
       "      <th>windoz</th>\n",
       "      <th>problemof</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9692</td>\n",
       "      <td>8505</td>\n",
       "      <td>12355</td>\n",
       "      <td>13322</td>\n",
       "      <td>14205</td>\n",
       "      <td>3081</td>\n",
       "      <td>16457</td>\n",
       "      <td>2236</td>\n",
       "      <td>12203</td>\n",
       "      <td>12999</td>\n",
       "      <td>...</td>\n",
       "      <td>14757</td>\n",
       "      <td>13675</td>\n",
       "      <td>7513</td>\n",
       "      <td>8672</td>\n",
       "      <td>3842</td>\n",
       "      <td>1976</td>\n",
       "      <td>1501</td>\n",
       "      <td>16825</td>\n",
       "      <td>12509</td>\n",
       "      <td>13851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 17594 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lc   iii    pop  reveal  socket  addit   vram    72    pin    ram  \\\n",
       "0  9692  8505  12355   13322   14205   3081  16457  2236  12203  12999   \n",
       "\n",
       "     ...     sympathi  samea  genlock  infam  auxiliari  64mb   432  windoz  \\\n",
       "0    ...        14757  13675     7513   8672       3842  1976  1501   16825   \n",
       "\n",
       "   problemof  selector  \n",
       "0      12509     13851  \n",
       "\n",
       "[1 rows x 17594 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(vectorizer.vocabulary_, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode the document\n",
    "tfidf_transformer=TfidfTransformer()\n",
    "tfidf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3903x17594 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 190613 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = tfidf_transformer.fit_transform(vectorizer_counts)\n",
    "vectorizer_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have converted the documents of 4 classes into numerical feature vectors by first tokenising each document into words and then excluding stop words, punctuations. \n",
    "\n",
    "Then after, we created TFxIDF vector representations. \n",
    "\n",
    "\n",
    "We will conclude by reporting the number of terms we extracted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Frequency: 2\n",
      "Number of Terms: 17594\n"
     ]
    }
   ],
   "source": [
    "print('Min Frequency: 2')\n",
    "print('Number of Terms: '+str(vectorizer_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3903, 17594)\n"
     ]
    }
   ],
   "source": [
    "#Summarize encoded vector\n",
    "print(vectorizer_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.22448033 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(vectorizer_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(vectorizer_tfidf.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding 10 most significant terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we found out most important terms/words in a document using TFIDF metric. We will use the same concept for finding out the most important term in a class for each subset: 'All', 'Train', 'Test'.\n",
    "* Class: \n",
    "    * comp.sys.ibm.pc.hardware\n",
    "    * comp.sys.mac.hardware\n",
    "    \n",
    "We have already install all the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining dictionary of subsets\n",
    "pretty_print={'all': 'All subsets included','train': 'Only training subsets included','test': 'Only testing subsets included'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_most_significant_helper_min_df_2(newsgroup,subset):\n",
    "    data=fetch_20newsgroups(subset=subset,categories=[newsgroup],shuffle=True,random_state=42,remove=('headers','footers','quotes')).data\n",
    "    punctuations='[! \\\" # $ % \\& \\' \\( \\) \\* + , \\- \\. \\/ : ; <=> ? @ \\[ \\\\ \\] ^ _ ` { \\| } ~]'\n",
    "\n",
    "    #preprocessing using the stemmer, and stripping off the punctuations\n",
    "    for i in range(len(data)):\n",
    "        data[i]=\" \".join([stemmer.stem(data) for data in re.split(punctuations,data[i])])\n",
    "        data[i]=data[i].replace('\\n','').replace('\\t','').replace('\\r','')\n",
    "    \n",
    "    #top 10 features required; also removing stopwords is necessary\n",
    "    counts=CountVectorizer(min_df=2,max_features=10,stop_words ='english')\n",
    "    X_counts=counts.fit_transform(data)\n",
    "    print('\\n')\n",
    "    print('Class being operated upon:'+newsgroup)\n",
    "    print('The data subset:'+pretty_print[subset])\n",
    "    print('10 most significant terms with min_df=2:')\n",
    "    \n",
    "    #Getting list of feature names\n",
    "    feature_names = list()\n",
    "    for i in counts.vocabulary_.keys():\n",
    "        feature_names.append(i)\n",
    "        \n",
    "    \n",
    "    for (term,count) in zip(feature_names,X_counts.toarray().sum(axis=0)):\n",
    "        spaces=''\n",
    "        for i in range(15):\n",
    "            if 15-i-len(term) > 0:\n",
    "                spaces += ' '\n",
    "        print(spaces+'\\\"'+term+'='+str(count))\n",
    "        \n",
    "    tfidf_transformer=TfidfTransformer()\n",
    "    class_tficf=tfidf_transformer.fit_transform(X_counts)\n",
    "    class_tficf_arr = class_tficf.toarray()\n",
    "    print('TFxICF dimension:',)\n",
    "    print(class_tficf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Spyder as your environment, you can easily get feature names using get_feature_names() method on CountVectorizer instance. But you can't do that in jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers for the 10 most significant terms in the total, train, and test subsets of the dataset    \n",
    "def ten_most_significant_min_df_2(newsgroup):\n",
    "    ten_most_significant_helper_min_df_2(newsgroup,'all')\n",
    "    ten_most_significant_helper_min_df_2(newsgroup,'train')\n",
    "    ten_most_significant_helper_min_df_2(newsgroup,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Class being operated upon:comp.sys.ibm.pc.hardware\n",
      "The data subset:All subsets included\n",
      "10 most significant terms with min_df=2:\n",
      "           \"card=448\n",
      "            \"use=505\n",
      "        \"control=397\n",
      "           \"know=367\n",
      "           \"disk=897\n",
      "          \"drive=299\n",
      "           \"work=332\n",
      "           \"scsi=580\n",
      "            \"ani=726\n",
      "        \"problem=335\n",
      "TFxICF dimension:\n",
      "(982, 10)\n",
      "\n",
      "\n",
      "Class being operated upon:comp.sys.ibm.pc.hardware\n",
      "The data subset:Only training subsets included\n",
      "10 most significant terms with min_df=2:\n",
      "          \"drive=279\n",
      "           \"card=194\n",
      "           \"work=331\n",
      "            \"ani=287\n",
      "        \"problem=277\n",
      "            \"bus=680\n",
      "            \"use=206\n",
      "           \"scsi=441\n",
      "        \"control=472\n",
      "           \"disk=193\n",
      "TFxICF dimension:\n",
      "(590, 10)\n",
      "\n",
      "\n",
      "Class being operated upon:comp.sys.ibm.pc.hardware\n",
      "The data subset:Only testing subsets included\n",
      "10 most significant terms with min_df=2:\n",
      "           \"work=169\n",
      "            \"use=174\n",
      "           \"know=217\n",
      "        \"problem=119\n",
      "            \"ani=115\n",
      "          \"drive=111\n",
      "           \"card=126\n",
      "             \"pc=139\n",
      "          \"modem=254\n",
      "           \"scsi=142\n",
      "TFxICF dimension:\n",
      "(392, 10)\n",
      "\n",
      "\n",
      "Class being operated upon:comp.sys.mac.hardware\n",
      "The data subset:All subsets included\n",
      "10 most significant terms with min_df=2:\n",
      "           \"work=341\n",
      "            \"doe=391\n",
      "           \"know=255\n",
      "           \"like=385\n",
      "        \"problem=271\n",
      "           \"appl=267\n",
      "            \"mac=629\n",
      "            \"ani=355\n",
      "          \"drive=564\n",
      "            \"use=293\n",
      "TFxICF dimension:\n",
      "(963, 10)\n",
      "\n",
      "\n",
      "Class being operated upon:comp.sys.mac.hardware\n",
      "The data subset:Only training subsets included\n",
      "10 most significant terms with min_df=2:\n",
      "           \"appl=177\n",
      "        \"problem=243\n",
      "           \"know=159\n",
      "           \"work=250\n",
      "            \"mac=163\n",
      "            \"use=164\n",
      "            \"ani=362\n",
      "           \"card=219\n",
      "           \"like=330\n",
      "          \"drive=159\n",
      "TFxICF dimension:\n",
      "(578, 10)\n",
      "\n",
      "\n",
      "Class being operated upon:comp.sys.mac.hardware\n",
      "The data subset:Only testing subsets included\n",
      "10 most significant terms with min_df=2:\n",
      "           \"know=164\n",
      "            \"ani=148\n",
      "           \"disk=118\n",
      "           \"file=135\n",
      "            \"mac=119\n",
      "            \"use=108\n",
      "        \"problem=267\n",
      "           \"appl=136\n",
      "          \"drive=234\n",
      "           \"work=134\n",
      "TFxICF dimension:\n",
      "(385, 10)\n"
     ]
    }
   ],
   "source": [
    "ten_most_significant_min_df_2('comp.sys.ibm.pc.hardware')\n",
    "ten_most_significant_min_df_2('comp.sys.mac.hardware')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
